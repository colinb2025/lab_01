{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi7xzykjgGP9"
      },
      "source": [
        "## Lab 1: Getting data from API's\n",
        "\n",
        "A great source of data and Pandas practice is getting data from the Internet. It is not going to come in a .csv file, though: It will be a stream of records, typically in XML (eXtensible Mark-up Language) or JSON (JavaScript Object Notation) format.\n",
        "\n",
        "We'll look at a very simple API and some useful code chunks for getting and analyzing data, and then you'll take a look at the APIs available from the Federal government as the main work for your lab.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "id": "TPF8GcllVQx5",
        "outputId": "4f9aa643-965a-406f-aeaa-96ab95e9d26a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.38.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.1.4)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.8.1)\n",
            "Collecting tenacity<9,>=8.1.0 (from streamlit)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog<5,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-4.0.2-py3-none-manylinux2014_x86_64.whl.metadata (38 kB)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Downloading streamlit-1.38.0-py2.py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading watchdog-4.0.2-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: watchdog, tenacity, smmap, pydeck, gitdb, gitpython, streamlit\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.43 pydeck-0.9.1 smmap-5.0.1 streamlit-1.38.0 tenacity-8.5.0 watchdog-4.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# API URL and query\n",
        "url = 'https://www.saferproducts.gov/RestWebServices/Recall'\n",
        "query = '?format=json&ProductType=Computer'\n",
        "\n",
        "# Fetch data using requests\n",
        "raw = requests.get(url + query)\n",
        "\n",
        "# Check if the API call was successful\n",
        "if raw.status_code == 200:\n",
        "    # Convert data to JSON\n",
        "    data = raw.json()\n",
        "\n",
        "    # Convert JSON data to DataFrame\n",
        "    df = pd.DataFrame.from_dict(data)\n",
        "\n",
        "    # Check if 'RemedyOptions' column exists\n",
        "    if 'RemedyOptions' in df.columns:\n",
        "        # Function to flatten dictionary column\n",
        "        def flatten_dict_column(df, column_name):\n",
        "            \"\"\"\n",
        "            Flatten a column with dictionaries/lists in the DataFrame and append the results to a new column.\n",
        "            \"\"\"\n",
        "            temp = df[column_name]\n",
        "            clean_values = []\n",
        "\n",
        "            for i in range(len(temp)):\n",
        "                if len(temp[i]) > 0:\n",
        "                    values = [entry['Option'] for entry in temp[i]]\n",
        "                    clean_values.append(values)\n",
        "                else:\n",
        "                    clean_values.append('')\n",
        "\n",
        "            return clean_values\n",
        "\n",
        "        # Flatten the 'RemedyOptions' column\n",
        "        df['remedy'] = flatten_dict_column(df, 'RemedyOptions')\n",
        "\n",
        "        # Get remedy counts\n",
        "        remedy_counts = df['remedy'].value_counts()\n",
        "\n",
        "        # Display results using Streamlit\n",
        "        st.title('Remedy Statistics for Computer Product Recalls')\n",
        "        st.write(remedy_counts)\n",
        "    else:\n",
        "        st.write(\"The 'RemedyOptions' field is not present in the data.\")\n",
        "else:\n",
        "    st.write(f\"Failed to retrieve data from the API. Status code: {raw.status_code}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "2INfBgG8U_gV",
        "outputId": "7139b839-aad4-4ca2-9a5c-9d17eda162a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-09-17 18:57:29.870 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-17 18:57:29.932 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2024-09-17 18:57:29.933 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-17 18:57:29.936 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2024-09-17 18:57:29.939 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py"
      ],
      "metadata": {
        "id": "878WNLfCWXpV",
        "outputId": "3345c227-503a-4c20-ef4a-24093d9904f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.189.189.131:8501\u001b[0m\n",
            "\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko09F0g3gGP-"
      },
      "source": [
        "## API Queries\n",
        "\n",
        "The core programming skill of the activity is to learn to query an online Application Programmer Interface (API). It is a similar experience to browsing the Internet, and initial results can be displayed the web browser. When visiting a web page, the address bar typically contains something like\n",
        "$$\n",
        "\\texttt{https://} \\underbrace{\\texttt{www.}}_{\\text{World Wide Web subdomain}} \\texttt{domain}. \\underbrace{\\texttt{tld}}_{\\text{Top level domain}},\n",
        "$$\n",
        "where the https:// specifies the protocol, www. specifies the world-wide-web subdomain, the domain is the entity, and the top-level domain .tld is typically something like .com or .gov, but is increasingly varied as ICANN releases more TLD's into circulation.\n",
        "\n",
        "With an online API, the user instead enters a url that goes directly to an API subdomain\n",
        "$$\n",
        "\\texttt{https://} \\underbrace{\\texttt{api.}}_{\\text{Application programmer interface}} \\texttt{domain.tld}/ \\texttt{(the query)}\n",
        "$$\n",
        "or accesses REST services as\n",
        "$$\n",
        "\\texttt{https://www.domain.tld} \\underbrace{\\texttt{/REST}}_{\\text{Accesses REST services}}/ \\texttt{(the query)}\n",
        "$$\n",
        "This accesses data on the domain's servers and returns the result directly to the user.\n",
        "\n",
        "The query itself is typically a string beginning with a question mark ?, followed by a series of expressions joined by ampersands &. For example,\n",
        "\n",
        "`?ProductType=Phone\\&Manufacturer=Apple`\n",
        "\n",
        "passes a query requesting all records for which the product type is recorded as phone and the manufacturer is recorded as Apple. Some API's include date ranges and other, more complex requests.\n",
        "\n",
        "To get started, a simple warm-up is to use the API from saferproducts.gov, which has a simple and intuitive structure for queries, and the results are simple enough to look at in the browser. Typing this in the address bar in a browser should yield about thirty records:\n",
        "\n",
        "    https://www.saferproducts.gov/RestWebServices/Recall?format=json&ProductType=Phone  \n",
        "\n",
        "with the first being, on this occasion:\n",
        "\n",
        "    \"RecallID\": 7856,\n",
        "    \"RecallNumber\": \"16266\",\n",
        "    \"RecallDate\": \"2016-09-15T00:00:00\",\n",
        "    \"Description\": \"This recall involves the Samsung Galaxy Note7 smartphone sold before\n",
        "    September 15, 2016. The recalled devices have a 5.7 inch screen and were sold in the\n",
        "    following colors: black onyx, blue coral, gold platinum and silver titanium with a\n",
        "    matching stylus. Samsung is printed on the top front of the phone and Galaxy Note7\n",
        "    is printed on the back of the phone. To determine if your phone has been recalled,\n",
        "    locate the IMEI number on the back of the phone or the packaging, and enter the IMEI\n",
        "    number into the online registration site www.samsung.com or call Samsung toll-free\n",
        "    at 844-365-6197.\",\n",
        "    \"URL\": \"https://www.cpsc.gov/Recalls/2016/Samsung-Recalls-Galaxy-Note7-Smartphones\",\n",
        "    \"Title\": \"Samsung Recalls Galaxy Note7 Smartphones Due to Serious Fire and Burn Hazards\",\n",
        "    \"ConsumerContact\": \"Contact your wireless carrier or place of purchase, call Samsung\n",
        "    toll-free at 844-365-6197 anytime, or go online at www.samsung.com.\",\n",
        "    \"LastPublishDate\": \"2016-10-27T00:00:00\"\n",
        "\n",
        "The query itself in this case is:\n",
        "\n",
        "    ?format=json&ProductType=Phone  \n",
        "\n",
        "The quert requests all of the recalls in JavaScript Object Notation (json) format, where the `ProductType` variable is equal to `Phone`. In addition to `ProductType`, other options include:\n",
        "\n",
        "    RecallID,\n",
        "    RecallNumber,\n",
        "    RecallDateStart,\n",
        "    RecallDateEnd,\n",
        "    RecallURL,\n",
        "    LastPublishDateStart,\n",
        "    LastPublishDateEnd,\n",
        "    RecallTitle,\n",
        "    ConsumerContact,\n",
        "    RecallDescription,\n",
        "    ProductName,\n",
        "    ProductDescription,\n",
        "    ProductModel,\n",
        "    ProductType,\n",
        "    InconjunctionURL,\n",
        "    ImageURL,\n",
        "    Injury,\n",
        "    Manufacturer,\n",
        "    Retailer,\n",
        "    Importer,\n",
        "    Distributor,\n",
        "    ManufacturerCountry,\n",
        "    UPC,\n",
        "    Hazard,\n",
        "    Remedy,\n",
        "    RemedyOption\n",
        "\n",
        "**1. Practice writing queries using the saferproducts.gov API and your web browser.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Zwfi6ndgGP-"
      },
      "source": [
        "## Accessing API's with Python\n",
        "\n",
        "Anytime you use a computer to access resources on the Internet, you will likely run into problems. There are many options, but two with low coding overhead: The `requests` and `urrlib.requests` packages.\n",
        "\n",
        "The following code chunk uses the `requests` package to get the same kind of data that was being displayed in the browser, but in an interactive Python session:\n",
        "\n",
        "    import requests\n",
        "    url = 'https://www.saferproducts.gov/RestWebServices/' # Location of the API\n",
        "    query = 'Recall?format=json&ProductType=Exercise' # The query\n",
        "    header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:124.0)\n",
        "        Gecko/20100101 Firefox/124.0'} # The user-agent to avoid being blocked\n",
        "    raw = requests.get(url+query,headers=header) # Query the database\n",
        "    data = raw.json() # Convert data from json to dictionary\n",
        "\n",
        "To make the code easier to read, it separates the url and the query into two different strings, then concatenates them in the GET request. This makes it easier to edit the query, as well as suggests a simple way to loop over a number of queries that might be sent to the same API.\n",
        "\n",
        "Many resources are designed to block access from particular kinds of users. In order to circumvent these obstacles, you can specify a `header` dictionary that presents the query to the server as coming from a hypothetical and common user. In this case, the header presents the query as coming from a Firefox browser from a Windows computer, rather than something like `python-requests/3.12.1`. This problem appears generally in scraping data from the web, and can grind the process to a halt. For whatever reason, I have been blocked and gotten 403 errors with the `requests` package, which motivated me to prepare a second alternative that seems more robust:\n",
        "\n",
        "    import urllib.request\n",
        "    import json\n",
        "    url = 'https://www.saferproducts.gov/RestWebServices/' # Location of the API\n",
        "    query = 'Recall?format=json&ProductType=Exercise' # The query\n",
        "    response = urllib.request.urlopen(url+query)\n",
        "    response_bytes = response.read()\n",
        "    data = json.loads(response_bytes) # Convert response to json\n",
        "    response.close()\n",
        "\n",
        "This is a bit more code and some steps are a bit less human-friendly, but seems to work a bit more reliably than `requests`.\n",
        "\n",
        "**2. Practice with the saferproducts.gov API and the above code in a notebook to see how API's work, in general.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWk9ceyRgGP_"
      },
      "source": [
        "## Wrangling the Data\n",
        "\n",
        "Piping the data to Pandas is easy, because the preceding code chunk put the JSON data into a native Python dictionary, and the following converts it to a dataframe:\n",
        "\n",
        "    df = pd.DataFrame.from_dict(data)\n",
        "\n",
        "If the data is in raw XML or JSON format --- which might especially be true with other API's --- it would instead be \\texttt{pd.read\\_xml(data)} or \\texttt{pd.read\\_json(data)}. Ironing out these details in advance for other applications is a key part of the presentation for students, but having students resolve these issues as a component of group work or an assignment is a great way to help them mature as coding problem solvers by struggling with documentation and a well-defined problem.\n",
        "\n",
        "Unfortunately, there aren't many non-text fields in the \\texttt{www.saferproducts.gov} data. However, there are a few fields of interest that can be tabulated and discussed, such as RemedyOptions and ManufacturerCountries:\n",
        "\n",
        "    df['RemedyOptions'].value_counts()\n",
        "\n",
        "with output\n",
        "\n",
        "    RemedyOptions\n",
        "    []                                                                               139\n",
        "    [{'Option': 'Repair'}]                                                            49\n",
        "    [{'Option': 'Replace'}]                                                           12\n",
        "    [{'Option': 'Refund'}]                                                             7\n",
        "    [{'Option': 'Replace'}, {'Option': 'Repair'}]                                      4\n",
        "    [{'Option': 'Refund'}, {'Option': 'Replace'}, {'Option': 'Repair'}]                1\n",
        "    [{'Option': 'Replace'}, {'Option': 'Refund'}]                                      1\n",
        "    [{'Option': 'Refund'}, {'Option': 'Repair'}]                                       1\n",
        "    [{'Option': 'Label'}]                                                              1\n",
        "    [{'Option': 'New Instructions'}, {'Option': 'Replace'}, {'Option': 'Refund'}]      1\n",
        "    Name: count, dtype: int64\n",
        "\n",
        "It's appropriate at this point to do some data cleaning, particularly by flattening dictionary entries. With response data that get converted from json to a dictionary, there are often values in the data frame that need to be flattened or unpacked. For example, some values are recorded as \\texttt{ [$\\{$'Country':'Canada'$\\}$]}, or, worse, a dictionary with multiple entries: \\texttt{[ $\\{$ 'Option': 'Replace'$\\}$, $\\{$'Option': 'Repair'$\\}$]  ] }. This can lead to problems when another package refuses to work with a lists of lists or doesn't know how to simplify a dictionary to data, and presents some conceptual questions when cleaning.\n",
        "\n",
        "A simple script to recursively collapse the dictionary entries into a single string is:\n",
        "\n",
        "    temp = df['RemedyOptions']\n",
        "    clean_values = []\n",
        "    for i in range(len(temp)):\n",
        "        if len(temp[i])>0:\n",
        "            values = []\n",
        "            for j in range(len(temp[i])):\n",
        "                values.append(temp[i][j]['Option'] )\n",
        "            clean_values.append(values)\n",
        "        else:\n",
        "            clean_values.append('')\n",
        "    df['remedy'] = clean_values\n",
        "\n",
        "**3. Convert this code chunk into a function you can reuse to flatten dictionaries, or explain clearly the problems you run into while attempting to do so. Make some tables or plots.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glTQ8P7ggGP_"
      },
      "source": [
        "## Dashboarding the Results (Optional)\n",
        "\n",
        "To complete the pipeline from data to product, we can use \\texttt{streamlit} to quickly convert Python code into a web page that can be accessed locally. This can be done with essentially three lines of code: An import statement, a $.title()$ method call to set the page title, and an $.write()$ call to push the results to the page. Although relatively static, completing this step serves a pedogogical and psychological purpose: It pivots the students to thinking about how to communicate results to an audience, and how the project could become an ongoing endeavor rather than a single analytical exercise.\n",
        "\n",
        "The entire .py file to create the dashboard is\n",
        "\n",
        "    import pandas as pd\n",
        "    import requests\n",
        "    import streamlit as st\n",
        "    # Conduct analysis:\n",
        "    url = 'https://www.saferproducts.gov/RestWebServices/Recall'\n",
        "    query = '?format=json&RecallTitle=Gas'\n",
        "    header = {'User-Agent':\n",
        "              'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:124.0) Gecko/20100101 Firefox/124.0'}\n",
        "    raw = requests.get(url+query,headers=header)\n",
        "    data = raw.json()\n",
        "    df = pd.DataFrame.from_dict(data)\n",
        "    temp = df['RemedyOptions']\n",
        "    clean_values = []\n",
        "    for i in range(len(temp)):\n",
        "        if len(temp[i])>0:\n",
        "            values = []\n",
        "            for j in range(len(temp[i])):\n",
        "                values.append(temp[i][j]['Option'] )\n",
        "            clean_values.append(values)\n",
        "        else:\n",
        "            clean_values.append('')\n",
        "    df['remedy'] = clean_values\n",
        "    remedy_counts = df['remedy'].value_counts()\n",
        "    # Create streamlit output:\n",
        "    st.title('Remedy Statistics')\n",
        "    st.write(remedy_counts)\n",
        "\n",
        "To create the web page, run the following at the command line:\n",
        "\n",
        "    streamlit run remedy.py\n",
        "\n",
        "This should convert the above analysis into a web page available from localhost.\n",
        "\n",
        "**4. Produce your own table or plot, and output it to streamlit.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ogbrOZYgGP_"
      },
      "source": [
        "## Other API Sources\n",
        "\n",
        "Valuable and interesting Federal API resources are listed at:\n",
        "\n",
        "    https://catalog.data.gov/dataset/?_res_format_limit=0&res_format=API\n",
        "\n",
        "and in the future will likely easily be found at \\texttt{data.gov}. Some highlights include:\n",
        "\n",
        "- CDC WONDER API for Data Query Web Service: Includes death certificates with causes since approximately the 1990's.\n",
        "- Comprehensive Housing Affordability Strategy (CHAS): Housing and Urban Development (HUD) maintains an API that provides Census data on housing problems and needs unavailable through other sources, including IPUMS.\n",
        "- Federal Election Commission API: Provides historical and up to the minute campaign finance data.\n",
        "- Toxic Release Inventory: Provided by the Environmental Protection Agency, this API documents the release and management of over 800 toxic substances, reported annually by privately owned facilities and the government.\n",
        "- Petroleum Data, Prices: Provides prices of petroleum products and crude oil at weekly, monthly, and yearly time scales.\n",
        "- Fair Market Rents Lookup tool: Fair Market Rents (FMRs) determine the value of housing vouchers for Section 8 renters. This API provides the FMR values and other measures of housing affordability.\n",
        "- Annual Economic Surveys, Business Patterns: Surveys of businesses at the zip code level, tracking economic sentiment and activity.\n",
        "- Food Access Research Atlas: Provides spatial data on food access and the availability of supermarkets within census tracts. Can be merged with census data to look at under-served populations and food deserts.\n",
        "- National Oceanographic and Atmospheric Administration: Provides API access to data on real time weather and climate change projections.\n",
        "\n",
        "Each of these API resources could either be the cornerstone of a project or a source of additional data. These data sources have a number of advantages: They're free, most of them can be accessed using the same API key, and most have similar documentation for how to write a query. This is ideal for students to iterate, experiment, and take risks, with little cost to failure.\n",
        "\n",
        "In addition to government data, many commerical apps provide API access to developers and researchers. AirBnB, Amazon, Reddit, eBay, X, and many others maintain API access to develop third-party apps. These opportunities present many advantages: The data are larger, have more variety, and there are vastly many more cases. Building a third-party app that includes analytics could easily consume an entire semester and open a variety of applications in predictive analytics, natural language processing, and generative AI (e.g. predict which reviews are fake or real for Amazon for a product group like ``women's watches', and then make recommendations for different price points). While an exciting possibility, this can also raise a lot of problems: Some API's cost money or are rate-limited depending on a subscription, and others impose significant constraints on how the data can be used. In some cases, a more useful approach might be explicit web scraping using a package like BeautifulSoup or Selenium. For example, Craigslist has no API, but can easily and productively be scraped using BeautifulSoup.\n",
        "\n",
        "**5. Pick an API, download some data, wrangle them, and produce some EDA results, as we did in the previous steps with the saferproducts.gov API; or, if you can't get it to work, document why. If you have the time and it's low cost, push the results to a streamlit page. If you have had enough, I recommend https://www.eia.gov/opendata/browser/electricity, since there is a friendly query builder that you can use to learn.**"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}